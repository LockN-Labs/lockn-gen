# 2026-02-04 Daily Memory

## 12:12 PM — Qwen2.5-VL-7B Vision Model Deployed ✅

**Task:** Deploy vision language model for image understanding / QA testing

**Completed:**
1. Downloaded model files from unsloth/Qwen2.5-VL-7B-Instruct-GGUF:
   - `Qwen2.5-VL-7B-Instruct-Q5_K_M.gguf` (5.1GB)
   - `mmproj-F16.gguf` (1.3GB vision encoder)
2. Tested with `llama-mtmd-cli` (deprecated `llama-qwen2vl-cli`)
3. Created systemd service `llama-qwen-vl.service` on port 11441
4. Verified OpenAI-compatible API with multimodal support

**Performance:**
- Prompt processing: ~2,925 tok/s (including image encoding)
- Generation: ~216 tok/s
- VRAM usage: ~12GB total

**API Usage:**
```json
{
  "messages": [{
    "role": "user",
    "content": [
      {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}
      {"type": "text", "text": "What do you see?"}
    ]
  }]
}
```

**Service:** `systemctl --user status llama-qwen-vl`

---

## Heartbeat 11:30 AM — LOC-110 Whitespace Fix

**Context Health:** 0/200K (0%) ✅ Fresh (cron isolated)
**Main Session:** 114K/200K (57%) ⚠️ Getting full

**LOC-110: lockn-listen Code Bug Fixes (In Progress)**
- PR #10: https://github.com/LockN-AI/lockn-listen/pull/10
- CI: Failed 3 times on whitespace formatting errors (dotnet format --verify-no-changes)

**Root Cause:** Code changes introduced formatting violations detected by CI's `dotnet format` check

**Fix Applied (commit 88bdf47):**
- Ran `dotnet format` via Docker container (SDK 9.0)
- Fixed 11 files with whitespace violations:
  - ApiKey.cs, ApiKeyService.cs, ApiKeyHttpClient.cs
  - ClassificationWebSocketHandler.cs, AudioChunkBuffer.cs
  - WhisperSttService.cs, OnnxSoundClassifier.cs
  - ApiKeyMiddleware.cs, ApiKeyValidationMiddleware.cs
  - InMemoryApiKeyRepository.cs, AuthEndpoints.cs

**CI Status:** Run queued (attempt 4)

**Active Tickets:**
- LOC-109: ML Runtime Layer spike - In Progress (awaiting Sean's A/B/C decisions)
- LOC-110: lockn-listen fix - In Progress (CI queued)
- LOC-105: CI/CD Workflows - In Progress (4/5 passing)

---

## Earlier Today

### Heartbeat 11:15 AM — LOC-110 Build Errors Round 3

**Context Health:** 0/200K (0%) ✅ Fresh

**LOC-110: lockn-listen Code Bug Fixes (In Progress)**
- PR #10: https://github.com/LockN-AI/lockn-listen/pull/10
- Previous CI run failed: 12 build errors

**Build Errors Identified:**
1. Missing `using LockNListen.Domain.Interfaces` in Program.cs
2. `WhisperOptions` type not found (TranscriptionEndpoints)
3. `ClassificationEndpoints` had dual `[FromBody]` (ASP0024 error)
4. `ClassificationWebSocketHandler` used non-existent WebSocket APIs:
   - `webSocket.Options` (doesn't exist)
   - `webSocket.AcceptAsync()` (WebSocket already accepted by middleware)
   - `result.Array` (use buffer directly)

**Fixes Applied (commit ef141c8):**
1. Added `using LockNListen.Domain.Interfaces` to Program.cs
2. Created `WhisperOptions` configuration class in Infrastructure.Services
3. Refactored ClassificationEndpoints to use `ClassifyRequest` DTO
4. Fixed WebSocket handler to use correct .NET WebSocket API

**CI Status:** Failed on whitespace formatting

**Value Rating:** 5.50/10 (proactive debugging, third iteration on same PR)

### Heartbeat 11:03 AM — LOC-110 Fix Pushed

**LOC-110: lockn-listen Code Bug Fixes (In Progress)**
- PR #10: https://github.com/LockN-AI/lockn-listen/pull/10
- Previous CI run failed: 2 errors (missing using statements)

**Fixes Applied:**
1. Added `using LockNListen.Domain.Models;` to:
   - `ClassificationEndpoints.cs`
   - `ClassificationWebSocketHandler.cs`

**Commit:** `02458eb` — pushed to loc-110-fix-code-bugs branch
**Value Rating:** 6.50/10

### Heartbeat 10:47 AM — LOC-110 Fix In Progress

**LOC-110: lockn-listen Code Bug Fixes (In Progress)**
- PR #10: https://github.com/LockN-AI/lockn-listen/pull/10
- CI: Running (2nd attempt)

**Fixes Applied:**
1. `OnnxSoundClassifier.cs`:
   - Use `Lazy<T>.Value` to access InferenceSession
   - Use `NamedOnnxValue.CreateFromTensor` for ONNX inputs
   - Fix byte[] → float[] conversion order (resample → convert → window)
   - Initialize `_modelPath` with `string.Empty` (nullable fix)

2. `WhisperSttService.cs`:
   - Convert byte[] to MemoryStream for ProcessAsync
   - Remove WithGpu() (GPU auto-enabled by Runtime.Cuda package)
   - Fix GetGgmlModelAsync signature (no CancellationToken)

### Earlier — LOC-109 Options Analysis

**Config issues fixed, but revealed deeper code bugs:**
- OnnxSoundClassifier: byte[] → float[], Lazy<T>.Run() → .Value.Run()
- WhisperSttService: byte[] → Stream, missing WithGpu(), wrong arg types

**Outcome:** Config issues fixed, LOC-110 created to track code bug fixes

---

## Heartbeat 8:34 PM — Dispatch Hang Diagnosis

**Context Health:** 0/200K (0%) ✅ Fresh (cron isolated)

**Issue Diagnosed: llama-coder-next dispatch hangs**
- Root cause: Server continues generating after client timeout disconnects
- Single-slot server (parallel=1) blocks all new requests until orphaned generation completes
- Task 7066 stuck generating 38/8000 tokens from abandoned Pac-Man request

**Impact:**
- LOC-111 implementation blocked (subagent couldn't dispatch)
- Any new dispatch requests will hang until zombie task completes

**Proposed Fixes:**
1. Add request cancellation on client disconnect (server-side)
2. Increase dispatch.sh timeout for large generations
3. Add health check before dispatch (skip if slot busy)

**CI Status:**
- lockn-listen: ✅ master green
- lockn-ai-platform: ✅ LOC-111 PR CI passing

**Active Work:**
- LOC-111: Pac-Man implementation pending (blocked by dispatch)
- No other open PRs


---

## 9:50 PM — LOC-111 Pac-Man Implementation Complete ✅

**Task:** Build Pac-Man clone with mobile swipe controls for dev.lockn.ai/pacman

**Blocker Encountered:** Dispatch timeouts (4+ failed attempts)
- `rapid-lagoon`, `grand-river`, `tidal-comet`, `young-ridge` all timed out
- Root cause: Main session had `/model coder` override → occupying Coder-Next's single slot
- With `--parallel 1`, new dispatch requests couldn't get a slot

**Resolution:**
1. Diagnosed deadlock via heartbeat cron (model override + single-slot collision)
2. Reset main session to default Opus via `/model default`
3. Restarted `llama-coder-next.service` to clear stuck generation
4. Re-dispatched with `CODER_MAX_TOKENS=2048 CODER_TIMEOUT=300`
5. Implementation succeeded in single iteration

**Files Created:**
- `web/pacman/index.html` — Full HTML+CSS+JS canvas game (~7KB)
- Updated `web/nginx.conf` — Added `/pacman/` route
- Updated `web/index.html` — Added nav link

**Commit:** e881591 on branch `loc-111-pacman`
**PR:** #6 (Phase 3 complete, awaiting review + deploy)

**Lesson Learned:** Never route main session to single-slot local model (Coder-Next). Reserve it exclusively for `dispatch.sh` tasks. Orchestration should stay on Opus.

---

## Key Learnings (2026-02-04)

1. **Single-slot deadlock:** With `--parallel 1` on Coder-Next, main session model override blocks ALL dispatch requests
2. **Fix pattern:** `/model default` + service restart to clear stuck tasks
3. **Prevention:** Keep main session on Opus; Coder-Next is dispatch-only
4. **Context-aware routing works:** 321K token context load handled locally on Coder-Next ($0 cost)

---

## Model Usage Summary (Evening Session)

| Model | Tokens In | Tokens Out | Use Case |
|-------|-----------|------------|----------|
| Qwen3-Coder-Next | 321K | 172 | Implementation dispatch |
| GPT-5.2-Codex | 152K | 7K | Architecture/review |
| Claude Opus 4.5 | 136 | 3.7K | Orchestration |

Local-first strategy working as designed — heavy lifting on Coder-Next, Opus for orchestration.
