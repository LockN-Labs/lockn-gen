# 2026-02-11 — Daily Activity Log

## 13:23 — Back Online
- Windows Update took system offline (Sean pinged at 10:37, 10:38, 10:40 with no response)
- Gateway cron unresponsive, llama.cpp 11437 down, billing error on a cloud proxy
- 5 items In Progress (over WIP limit of 3), 6 PRs in Review queue
- Docker containers up 14 min (auto-started after reboot)

## 13:25 — Initial Actions
- Spawned dev agent on LOC-421 (auth-to-checkout bridge) — completed recon pass, has implementation plan
- Reviewed project-level priority queue from Linear
- Composed ranked/bucketed improvement list (4 buckets, 13 items)

## 13:45 — Bucket 1 Execution
- Gateway restart attempted — tool-based restart disabled in config, systemd restart kills my own session
- llama-qwen.service (Qwen3-32B, port 11437) started successfully — was not auto-enabled after reboot
- All 20 Docker containers already up and healthy (14 min uptime at check)
- Cron subsystem still broken — gateway HTTP is 200 but cron tool times out. Needs manual `systemctl --user restart openclaw-gateway.service`

## 13:55 — Communication Failure Post-Mortem
- Sean had to ask twice for updates — my responses were going into session but not always reaching Slack
- Root cause: relying on session auto-routing instead of explicit `message send`
- Added communication rules to AGENTS.md: always ack immediately, always explicit Slack send, never go silent >2 min

## 14:02 — Gateway Config & Update
- Enabled `commands.restart = true` in gateway config (was disabled, preventing self-healing)
- OpenClaw updated from 2026.2.6-3 → 2026.2.9 (Sean ran manually)
- Cron subsystem STILL broken after update + restart

## 14:14 — Slack Message Ordering Issue
- Sean flagged messages arriving in reverse order in Slack
- Root cause: sending multiple fragments during investigation, Slack timestamps them out of order
- Added rule to AGENTS.md: one composed message per update, no fragments

## 14:25 — Cron Root Cause Found
- 46 cron jobs (31 enabled, 15 disabled), 70KB jobs.json, 2.9MB run history
- Cron list tool times out at 60s trying to serialize all of it
- From the Feb 8-11 monitoring explosion (30+ observation crons created)
- Proposed: nuke and rebuild with only essential jobs (heartbeat, work-executor, idle-watchdog, daily-summary)
- Waiting on Sean's approval

## Key Infrastructure State
- llama-qwen (Qwen3-32B, port 11437): restored, was not auto-enabled after reboot. Service is `disabled` in systemd — should enable for boot persistence
- llama-coder-next (port 11439): running, healthy
- llama-coder-cpu (port 11440): running (Q3_K_M CPU-only, 2M context)
- All 20 Docker containers: healthy
- Gateway: running on 2026.2.9, cron subsystem broken
- Billing error: saw `{model}` provider billing error in Slack — unresolved, likely Ollama cloud proxy

## 15:00 — Gateway Session Pruning
- Root cause of 84% gateway CPU: sessions.json ballooned to 29MB / 2,255 entries (2,115 stale cron run sessions)
- Session folder: 467MB / 3,787 files
- Pruned to: 7.6MB / 488 entries, 155MB / 381 files (deleted 3,407 files)
- Gateway CPU: 84% → 21%, cron list works again
- Need recurring session pruning cron to prevent recurrence

## 15:10 — LockN Control Project Kickoff
- Sean requested Grafana/Prometheus observability platform → proper project kickoff
- Spawned 5 stakeholder review agents (eng, devops, finance, marketing, product)
- 4/5 completed successfully; product review hit Codex billing error
- Created Linear project "LockN Control" with 3 milestones, 15 tickets (LOC-482–496)
- Brief at projects/control/brief.md
- Finance: conditional go, $540 MRR projected at 6mo, 4-6mo break-even
- Engineering: high feasibility, 7-week full v1
- Key risk: no official subscription cap APIs (must infer from usage)
- Codex billing error confirmed — OpenAI credits likely exhausted

## 15:15 — Pre-Compaction Memory Flush

### Gateway Session Bloat (Root Cause Found & Fixed)
- sessions.json at `/home/sean/.openclaw/agents/main/sessions/sessions.json` ballooned to 29MB / 2,255 entries
- 2,115 were stale cron run sessions (null origin) never cleaned up
- Total session folder: 467MB / 3,787 files
- Pruned to: 7.6MB / 488 entries, 155MB / 381 files
- Gateway CPU dropped 84% → 21%
- Created daily session-pruning cron (job ID: 8a6aaef5) to prevent recurrence
- Backup at: sessions.json.bak.pre-prune

### LockN Control Project — Full Kickoff Complete
- Sean initiated: wants Grafana + Prometheus for model usage, infra health, cap tracking
- Ran pm-kickoff skill: 5 stakeholder agents (eng/devops/finance/marketing/product)
- 4/5 completed; product review hit **Codex billing error** (OpenAI credits likely exhausted)
- Linear project created: LockN Control (ID: d73dec54-b377-47cd-9cde-6c5fe9305399)
- 3 milestones: M1 Foundation (Mar 11), M2 Hardening (Mar 25), M3 Alpha Launch (Apr 22)
- 15 tickets: LOC-482 through LOC-496, 49 total story points
- Brief: projects/control/brief.md
- Finance verdict: CONDITIONAL go — must hit $250 MRR in 60 days post-launch or freeze
- Key architecture: Prometheus backbone, 3 ingestion lanes (native exporters, custom SQLite exporter, quota exporter)
- Ports: Grafana 3300, Prometheus 9090, Alertmanager 9093
- First tickets to start: LOC-482 (metrics schema) → LOC-483 (deploy stack)

### Codex Billing Error — Confirmed Provider
- The `{model}` billing error seen in Slack is **OpenAI Codex** (gpt-5.3-codex)
- Product review subagent failed with: "API key has run out of credits or has an insufficient balance"
- Multiple cron jobs use codex model — they'll all fail until credits are topped up
- Sean needs to check OpenAI billing dashboard

### pm-kickoff Model Selection Issue
- All 5 stakeholder agents spawned without explicit `model` param
- Default subagent model: `llamacpp-coder-next/qwen3-coder-next-q5k` (local)
- Fallback chain: Coder-Next → Sonnet → Kimi → **Codex** → Qwen3-32B
- 5 parallel spawns likely overwhelmed the single GPU, cascading most to Codex via fallbacks
- Sean noticed and asked why — answer: skill doesn't specify per-role models, needs update
- **TODO**: Update pm-kickoff SKILL.md to assign explicit models per stakeholder role and stagger spawns

### LockN Control — Dashboard Vision & Figma
- Sean wants an always-on "mission control" dashboard on a separate monitor
- 3-tier layout: Alerts/Blockers (top) → Active Threads + System Status (middle) → Priority Queue (bottom)
- Approach chosen: **C (Hybrid)** — Grafana backbone + custom intelligence panels for Linear/agent status
- UX vision doc completed: `projects/control/ux/dashboard-vision.md`
- Figma mockups blocked — Desktop Bridge plugin won't connect from subagent sessions (port 9223 conflict). Tried twice, same issue. May need to investigate MCP server config or run from main session.
- Sean wants Figma mockups done before Grafana implementation

### LockN Tune — Fine-Tuning Pipeline
- Sean noticed it's missing from Linear projects
- Found in business plan: user-friendly fine-tuning (LLMs + TTS), LoRA/QLoRA, no ML expertise needed
- Planned for Jul–Aug 2026
- Asked Sean if he wants scaffold (A) or full kickoff (B) — awaiting response

### pm-kickoff Skill Updated
- Added explicit model assignments per stakeholder role (was defaulting to subagent chain → cascading to Codex)
- Engineering→Codex, DevOps/Finance→DeepSeek, Marketing/Product→Sonnet
- Two-wave spawning to avoid GPU contention (Wave 1: eng+devops+finance, Wave 2: marketing+product)
- Sean confirmed this ties into broader C-Suite/system improvements planned

### LockN Tune — PM Kickoff Complete (15:50)
- All 5 stakeholder reviews completed (eng on Codex, devops+finance on DeepSeek, marketing+product on Sonnet)
- Wave pattern worked: Wave 1 (eng/devops/finance) → Wave 2 (marketing/product)
- DevOps and Finance initial runs failed (workspace path issue — cloud agents couldn't find workspace). Re-runs with explicit file paths worked.
- Engineering: feasibility HIGH, 10 weeks, control-plane + worker arch, PyTorch/PEFT/HF
- Finance: CONDITIONAL GO, $72K MRR at 12mo, Starter $29/Pro $79/Enterprise $299
- Marketing: Creator-led GTM, "Own your AI models" positioning
- Product: RICE 64, 9 MVP features, 4 milestones
- Linear project created: LockN Tune (ID: c0291d45-eb04-4fef-af2c-85a94dfc958c)
- 4 milestones: M1 Foundation (Mar 15), M2 Core Pipeline (May 1), M3 TTS Integration (Jun 15), M4 MVP Launch (Aug 1)
- 12 tickets: LOC-497 through LOC-508, 69 story points
- Brief: projects/tune/brief.md
- Reviews: projects/tune/reviews/ (5 files: engineering, devops, finance, marketing, product)

### Figma Bridge Issue — Subagent Access Blocked
- UX subagents cannot connect to Figma bridge (port 9223 EADDRINUSE)
- The MCP figma-console server itself holds port 9223, so subagent sessions can't reconnect
- Tried 3 times with different spawns — same result every time
- HTML/CSS fallback mockup created at projects/control/ux/dashboard-mockup.html
- Sean asked what we need to do to fix this — needs investigation
- Likely fix: either the MCP server needs to share the connection, or we need a different bridge architecture

### Figma Bridge — Port Proxy Hardening (16:00 EST)
- Sean set up Windows `netsh portproxy` forwarding localhost:9223 → WSL IP:9223
- Port proxy rules persist across reboots, but WSL IP changes on reboot → proxy breaks silently
- Created comprehensive runbook: `docs/figma-integration-runbook.md`
  - 5-link chain diagram with survival matrix
  - PowerShell script for re-running proxy after reboot
  - Optional scheduled task for auto-run at login
  - Troubleshooting table
- Updated TOOLS.md with proxy details, subagent limitation, startup sequence
- MCP server (PID 1968) is listening on 9223, port proxy is configured
- Bridge plugin activated by Sean but connection NOT confirmed yet
  - `figma_list_open_files` returns empty — bridge may not have connected
  - Possible issue: portproxy listenaddress mismatch (0.0.0.0 vs 127.0.0.1)
  - Asked Sean to verify `netsh interface portproxy show v4tov4` and bridge plugin status
- **STILL BLOCKED**: Need Sean to confirm bridge shows "Connected" in Figma UI

### Cron System Status (Post-Pruning)
- 31 enabled cron jobs, all responding
- Cron list no longer times out
- Session pruning cron added (runs 3am daily)
- Usage collector still running every 5min (job: b32f23f6)
- Work executor hourly still active (job: 84b10703)
