# Daily Memories - 2026-02-02

## Revenue Goal
- **February 2026 target:** $500 via AI-assisted income streams
- Ramp-up month â€” building toward consistent revenue
- **Strategy focus:** Prediction markets as primary passive income approach
- Also considering crypto markets as secondary option
- Research spawned: platform comparison, edge strategies, capital requirements, automation

## Infrastructure Strategy
- **Priority shift:** Maximize local model capability before pursuing revenue
- Focus: concurrent multi-model inference on 96GB VRAM, cost-controlled routing
- Cloud subscriptions (Claude Code, OpenAI Codex) reserved for max-ROI tasks only
- Research spawned: best local models for agentic work, multi-model VRAM planning, routing framework
- **Architecture decision:** Drop Ollama for inference, use llama.cpp (GLM) + vLLM (Qwen3-32B), Ollama for embeddings only
- **New workstream:** Evaluate OpenClaw for fork/productization opportunities (LockN Labs R&D angle)
- Potential products: smart model router, multi-instance gateway, local inference manager, FinOps layer

## LockN Infer Orchestration Layer (2026-02-02)
- **Name:** LockN Infer
- **Default Orchestrator:** Claude Opus 4.5 (via OpenClaw)
- **Strategy:** Claude Opus intelligently routes tasks and spawns local subagents via `sessions_spawn`
- **Token Preservation:** 95% of tasks handled by local models (GLM-4.7-Flash, Qwen3-32B)
- **Cloud Cost:** ~95% reduction (from $100-300/month to <$30/month)
- **Key Insight:** Claude Opus handles *orchestration*, not *execution*. Local models handle the heavy lifting.
- **Architecture flow:** Claude Opus analyzes task â†’ spawns GLM/Qwen3 subagents â†’ returns results
- **Implementation:** Update openclaw.json to keep Claude Opus as primary, configure local models as subagent options

## LockN Labs Product Suite (Emerging)
1. **LocknReceipts** â€” tool usage receipt logging (Postgres + MinIO + FastAPI). Full spec at `research/lockn-receipts-spec.md`
2. **LocknFinOps** â€” OTel Collector processor + cost attribution dashboards. Gap: nobody owns agent FinOps. Research at `research/otel-finops-research.md`
3. **LocknInference** â€” local inference manager ("systemd for LLMs"). Deepest moat. Research at `research/openclaw-product-opportunities.md`
- Strategy: Adjacent + Contribute (don't fork OpenClaw, build framework-agnostic tools)

## Model Downloads Complete
- GLM-4.7-Flash Q6_K XL: 24.3GB â†’ ~/models/text/
- Qwen3-32B Q5_K_M: 21.6GB â†’ ~/models/text/
- Ready for dual llama.cpp setup

## All Research Deliverables
- `research/prediction-markets-feb2026.md`
- `local-llm-stack-2026.md`
- `ARCHITECTURE.md`
- `research/openclaw-skills-audit.md`
- `research/vllm-setup-guide.md`
- `research/openclaw-product-opportunities.md`
- `research/otel-finops-research.md`
- `research/lockn-receipts-spec.md`
- `research/openclaw-enhancement-ideas.md`

## Next Steps (when Sean is ready)
1. Stand up dual llama.cpp instances (GLM on :11436, Qwen3-32B on :11437)
2. Update openclaw.json â€” default to GLM, add Qwen3 provider
3. Begin LocknReceipts Phase 1 (Postgres schema + API)

## Sean's Requests
- **Heartbeat Frequency Change**: Sean requested that the heartbeat frequency be changed from every minute to every 30 minutes
- **Monitoring Requirement**: Sean specified that monitoring should focus on actual limits rather than status.claude.com

## Current HEARTBEAT.md Configuration
The heartbeat system is configured with three main tasks:
1. Review open threads/workstreams, track open tasks, and report progress/updates/blockers Sean should hear about
2. Do lightweight research relevant to our goals; capture key learnings for the daily 7am summary
3. For any issue encountered, identify root cause(s) and proactively drive fixes/mitigations; surface the root cause and plan in the heartbeat update

## Heartbeat Check - 2026-02-02 12:21 EST

### âœ… Completed Tasks
- **Heartbeat Frequency Updated**: Changed from 1-minute to 30-minute frequency
  - Job ID: `98fa273f-ed92-49a2-816f-2ac819feba77`
  - Next run: 2026-02-02 12:51 EST

### ðŸ“Š Open Workstreams
- **Total Active Sessions**: 13
  - 3 High Priority (go ahead, where are we now, approval)
  - 7 Medium Priority (D0AC46HCRNZ threads)
  - 2 Low Priority (openai:ad5e1850, slack:#all-lockn-ai, slack:g-d0ac46hcrnz)
- **Recommendation**: Review high-priority threads for relevance

### âš ï¸ Issues Identified
1. **Port 8080 Spontaneous Stop**: ðŸŸ¡ Needs Investigation
   - Occurred: 2026-02-01 ~21:01
   - No crash logs found
   - Restarted at 21:41
   - No recurrence in 10+ hours
   - Recommendation: Monitor for recurrence

2. **Codex Status Check Failed**: â“ Unable to check
   - Command: `codex /status`
   - Error: "stdin is not a terminal"
   - Recommendation: Try alternative method

### ðŸ“ Next Steps
1. Review high-priority threads (sessions cd04bfe6, 3c6f0562, 144cef26)
2. Monitor Port 8080 for spontaneous stops
3. Find alternative method to check Codex status

---

## ðŸ§  Research Findings (Heartbeat 2026-02-02)

### Lightweight Research Topics Identified
Based on MEMORY.md goals, potential research topics for daily 7am summary:

1. **Qwen Models vs Gemma-2-27B**: Performance benchmarks and production considerations
2. **Q4_K_M Quantization**: 75% memory savings with ~5% quality loss
3. **Vector Database Comparison**: Qdrant vs LanceDB vs Milvus vs Weaviate
4. **MCP/A2A Protocols**: Latest developments and implementation patterns
5. **Agentic AI Governance**: FinOps for agents and cost management strategies

### llama.cpp Server Status Review
- Port 11436: âœ… Healthy (PID: 34679, 23.2% CPU, 8.4GB RAM)
- Port 8080: âœ… Healthy (PID: 56734, 0.0% CPU, 774MB RAM)
- **Observation**: Port 8080 spontaneous stop requires continued monitoring

### Open-Source LLM Ecosystem
- 2026 hardware guides emphasize Q4 quantization for local deployment
- Minist series hardware requirements well-documented (3B: 3GB VRAM, 8B: 8GB VRAM, 14B: 16-18GB VRAM)
- Devstral 2 Small 24B requires 32GB VRAM (advanced applications)

---

*Updated: 2026-02-02 12:21 EST*
## âš ï¸ Lesson Learned: USE THE CODING PIPELINE (16:57 EST)
- **Problem:** LOC-11 was sent to Codex for full implementation â€” burning cloud tokens on work Qwen3-Coder handles fine locally.
- **Root cause:** I forgot to use the `coding-pipeline` skill we built together. The skill exists, dispatch.sh works, Qwen3-Coder is running on :11438. I just... didn't use it.
- **Rule going forward:** ALL implementation/coding tasks go through `coding-pipeline` skill â†’ `dispatch.sh` â†’ Qwen3-Coder (:11438). Opus/Codex ONLY architects and reviews. No exceptions.
- **Pattern:** Opus architects â†’ dispatch.sh sends to Qwen3-Coder â†’ Opus reviews â†’ assembles PR
- **Token savings:** ~90% of coding tokens stay local (zero cloud cost)

## Standing Order: Full Autonomy (17:34 EST)
- Sean explicitly stated: "I'm the owner of the engineering company, nothing too hands-on."
- Pipeline should run without Sean in the loop for: PR reviews, merges, ticket transitions, kicking off next tickets.
- Only escalate for: strategic decisions, scope changes, unexpected blockers needing human judgment.
- Updated: HEARTBEAT.md, MEMORY.md, coding-pipeline SKILL.md

## Process: Linear Dependency Management (15:01 EST)
- Sean wants predecessors/successors set on all Linear work items (epics, stories, tasks)
- Goal: query for items where all predecessors are complete â†’ auto-dispatch to subagents
- Enables parallel execution when multiple items become unblocked
- Add a Linear "ready to pick up" query to heartbeat checks
- Scale processing output to maximize hardware resources (32c/64t Threadripper, 96GB VRAM)

## LockN Voice MVP â€” Initiated (20:41 EST)

- New project: LockN Voice â€” voice cloning & TTS wrapping NVIDIA Magpie-TTS
- GitHub: LockN-AI/lockn-voice (private)
- Linear project: "LockN Voice" (8 tickets: LOC-23 through LOC-30)
- .NET solution scaffolded: Api / Domain / Infrastructure
- Architecture: NeMo (Apache 2.0) + Magpie-TTS Zeroshot (NVIDIA Open Model License)
- Crons created:
  - `lockn-logger-backlog-brainstorm` â€” 6am daily, Opus, isolated
  - `lockn-voice-pipeline-5min` â€” every 5 min, Opus, isolated (autonomous pipeline)
- Dev agent spawned for LOC-23 + LOC-24 (parallel, no dependencies)
- LOC-23: NeMo Docker inference server (FastAPI + Magpie-TTS)
- LOC-24: Voice Profile domain model + PostgreSQL (EF Core)
- Sean stepped away â€” pipeline running autonomously overnight

## New Project Idea: LockN Swap
- **Concept:** Arbitrage bot on XRP Ledger
- **Status:** Idea phase â€” Sean wants to start noodling after LockN Voice wraps
- **Notes:** XRPL has native DEX with order books + AMM pools, low tx fees (~0.00001 XRP), fast settlement (~3-5s). Good candidate for on-chain arbitrage (cross-pair, AMMâ†”orderbook, cross-exchange).
- **Next:** Research when Sean's ready â€” XRPL DEX mechanics, existing OSS arb bots, architecture sketch


## Life Principles Adopted (01:31 AM)

Sean shared alignment principles he's considering adopting â€” stored in USER.md. Core theme: internal coherence over performance. Actions matching values without explanation. "You don't need to become someone new. You need to stop contradicting who you already are."

