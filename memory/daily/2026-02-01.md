# memory/2026-02-01.md - Daily Notes

## 2026-02-01 (Friday)

### llama.cpp Build & Monitoring Session

**Project:** llama.cpp (LLM inference framework) - Session ID: `warm-nudibranch`

**Completed Work:**

1. **llama.cpp Build - COMPLETED**
   - Successfully built llama.cpp on Threadripper Pro server
   - Active instances running:
     - `llama-server` on port 11436 (PID 34679): Running since 20:23
     - `llama-server` on port 8080 (PID 36439): Running since 21:41 (restarted after spontaneous stop at ~21:01)
     - `llama-cli` (PID 36530): Processing prompt since 21:02
     - `ollama serve` (PID 21720) via node proxy
   - GPU utilization stable (~44%)

2. **Documentation Created**
   - `memory/llama.cpp-openclaw-guide.md` - Complete integration guide including:
     - OpenAI-compatible API configuration (port 11436)
     - Production deployment recommendations (systemd vs Docker)
     - CUDA optimization settings
     - Model recommendations (GLM 4.7 Flash Heretic, Phi-4 14B, Gemma3 27B, Mistral Small 3.1 24B)
     - Monitoring and observability best practices

3. **Research Completed**
   - llama.cpp + OpenClaw compatibility confirmed
   - Production best practices documented
   - Multi-instance management options (AI Server gateway)

**Issues Identified & Mitigated:**
- Port 8080 server stopped spontaneously around 21:01 with no crash logs
- Root cause unknown; service restarted at 21:41
- No errors in logs after restart; stable operation confirmed

**Open Tasks:**
- Awaiting user decision on production deployment approach
- Model selection for production environment (GLM 4.7 vs alternatives)
- Systemd service setup for production

**Heartbeat Protocol:**
- Following HEARTBEAT.md protocol for periodic monitoring
- Silent monitoring with proactive issue identification
- Daily heartbeat checks completed

**Latest Check (2026-02-01 23:22):**
- All services stable and running
- Port 11436: Running since 20:27, CPU 40.2%
- Port 8080: Running since 21:44 (restarted after earlier stop), successfully processed chat request (200 status), no errors in logs
- ollama services: Running normally
- No new issues detected

**Latest Check (2026-02-01 23:24):**
- All services stable and running
- Port 11436: Running since 20:27, CPU 40.2%
- Port 8080: Running since 21:44, idle, no errors in logs

**Latest Check (2026-02-01 23:57):**
- All services stable and running
- Port 11436: Running since 20:28, CPU 36.0%
- Port 8080: Running since 21:45, idle, no errors in logs
- ollama services: Running normally
- No new issues detected since last check
- ollama services: Running normally
- No new issues detected

**Latest Check (2026-02-01 23:25):**
- All services stable and running
- Port 11436: Running since 20:27, CPU 40.1%
- Port 8080: Running since 21:44, idle, no errors in logs
- ollama services: Running normally
- No new issues detected

**Latest Check (2026-02-01 23:26):**
- All services stable and running
- Port 11436: Running since 20:27, CPU 39.9%
- Port 8080: Running since 21:44, idle, no errors in logs
- ollama services: Running normally
- No new issues detected

**Model Comparison Research (2026-02-01, 22:49 EST)**
- **GLM 4.7 Flash Heretic:** 30B-A3B MoE model, Q4 quantization ~17GB
  - Performance: ~112 tok/s on RTX 6000 Ada
  - Issues: Some users report "memorization bot" behavior on simple tasks; Q4 quantization can loop on basic questions
  - Recommendation: Keep for general use, test Phi-4 for coding/complex tasks
- **Phi-4 14B:** Best balance for most use cases (smaller, faster, best benchmarks)
- **Gemma3 27B:** Good alternative for strong reasoning
- **Mistral Small 3.1 24B:** Good all-rounder, outperforms GPT-4o mini

**Production Recommendations:**
- Use llama.cpp over Ollama for production (1.8x faster, more control)
- Native build with systemd recommended for production
- Phi-4 14B recommended as primary model for production deployment

**Next Priority:**
- User decision needed on production deployment approach
- Model selection for actual production use

**Heartbeat Check Summary (2026-02-01, 23:18 EST)**
- **Status:** All services running stably
- **Port 11436:** Running since 20:27, GPU utilization 40.4%
- **Port 8080:** Running since 21:44 (restarted after earlier stop), no errors in logs
- **ollama services:** Running normally
- **Monitoring:** Continuing silent monitoring with proactive issue identification and mitigation
- **No new issues detected** since last check

**Heartbeat Check Summary (2026-02-01, 23:56 EST)**
- **Status:** All services running stably
- **Port 11436:** Running since 20:28, CPU 36.0%
- **Port 8080:** Running since 21:45, idle, no errors in logs
- **ollama services:** Running normally
- **Monitoring:** Continuing silent monitoring with proactive issue identification and mitigation
- **No new issues detected** since last check
